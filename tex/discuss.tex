\section{Performance}

\figref{performanceNorm} together with the statistical data in table \ref{score2} showed that there is a statistical difference in performance when controlling the ROV with and without predictive help. In addition, the effect size of $0.904$ can be categorized as a medium to large effect. Especially when considering how easy and cheap this predictive method is to implement. The subjects performed on average 20.6\% better. This can be a valuable increase in situations were performance degradation due to communication latency is a problem.

Past research, table \ref{reviewPred}, describes a wide range performance gains from predictive technology. Everything from 8\% to 65\%. It's hard to do a direct comparison to any specific experiment, but a performance increase of 20.6\% in this experiment is probably in the lower range of what has been found before. What is true though, is that the predictive method described in this thesis is the cheapest and easiest to implement. This holds true when comparing to the experiments in the mentioned table.

None of the subjects were told that there would be a predictive display or how it worked. Some of the participants immediately identified what the predictive display was trying to tell them. Others however, didn't understand that there had been a predictive display until the experiment was over. The ones who tried to use the predictive display the way it was intended typically performed better than those who didn't. It may be possible that we could have seen an improved performance, if the subjects were informed how the predictive display works. This can however not be verified unless additional experiments are performed.

It is also interesting the evaluate if participants had any learning effects during the experiment. \figref{learning_effect} shows the score for each display type and further divided into groups depending if the subject had that display as the first, second or third display. As an example, the first of the nine box plots describes the score achieved in the delay condition for those who had that display as their first display. One visible trend is that the participants who had a particular display as their second display, performed better than those who had that display as their first. This performance increase was significant for all displays. Delay: t(18)=2.19, p=0.042, d=0.671, delay PD: t(17)=2.19, p=0.043, d=0.660, no delay: t(17)=3.26, p=0.005, d=0.902. The performance change from \#2 to \#3 in all displays were however not found significant.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{learning_effect}
    \caption{Score categorized after display order.}
    \label{learning_effect}
\end{figure}

The participants were also asked how much computer games they play. \figref{gamer_performance} shows the score in each display type for gamer (G), versus non gamer (NG). Gamers are defined as those who play video games weekly or more and they made up 30\% of the total experiment group. It is interesting to see that G's only performed better than NG's in the second and third condition. PD: t(26.57)=2.23, p=0.034, d=0.692 and no delay: t(40.79)=2.56, p=0.014, d=0.660.

By looking at the performance difference between a delayed display without and with predictive technology. It also becomes clear that gamers had a bigger gain using the predictive display. While NG's had a moderate effect size of 0.577, t(39)=3.27, p=0.002, d=0.577. Gamers had almost twice the effect size of 1.018, t(16)=4.17, $p<.001$, d=1.018. Exactly why G's were able to increase their performance more using the PD is unclear. It could be that the arrow in the PD which acts like an aiming device, is a more familiar concept for gamers.



\section{Subjective measurements}

\figref{tlx} showed minimal differences between the task load index metrics when condition one and two were compared. The only significant differences were found in the performance and frustration metric. Subjects felt that they on average performed 14\% better using the predictor display, t(56)=3.24, p=0.002, d=0.360. The actual performance increase was 21\%. They also reported that they felt 11\% less frustrated using the PD, t(56)=2.15, p=0.036, d=0.271. Participants reported that the no delay condition was better in all metrics, with an exception of \emph{temporal demand} where the difference was not significant.

Since the participants reported less frustration using the PD, it it interesting to look at how frustration an subjective delay time might be related. \figref{delay_vs_frustration} shows a scatter plot of reported frustration and delay time for all conditions collectively. All values has been normalized. The linear relationship is minimal at best and no conclusions can be drawn from the data.

\todo[inline]{
does not say cause, maybye drop this, thin ice
}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.65]{delay_vs_frustration}
    \caption{Normalized subjective delay versus frustration}
    \label{delay_vs_frustration}
\end{figure}


\section{Future work}

Although the PD increased performance, many participants experienced minor improvements. In addition, some of the subjects even reported that the PD was distracting and confusing. I believe that there are two main reasons to this. \figref{predictorvis} at page \pageref{predictorvis} shows the implemented PD.

Firstly, the fact that the image itself is moving around and scaling up and down constantly while the operator are using the ROV is distracting in itself. It's very easy that the operators attention is distracted because of all the activity happening on the screen. A good approach could be to incorporate something similar to \citep{Baldwin1999} who used cropped video from a panoramic camera. By only displaying parts of the FOV and changing this selection in response to operator controls, the video would not have to move around on the screen. The PD algorithm in this thesis can easily be altered to this kind of behavior. The disadvantages of such a method is that it would require a wider FOV camera which is typically more expensive. In addition, by only displaying part's of the video the displayed resolution will drop. This can be accommodated by sending a higher resolution image, but this would require more processing power and possibly increase the video latency.

Secondly, many of the operators used the physical black peg as visual guidance even though the red arrow was included. This meant that the operator frequently overshot the target and in practice didn't use the predictor. In this experiment the operator had to move this peg into holes and it therefore had to be visible in the video. If the task had been to maneuver an obstacle course the peg could be removed. Then the subjects only visual aid would then be the red arrow. This would presumably make them use it much more, and don't overshoot targets as much.

Although the predictive method is model free and can be used on all maneuvering ROV's. The pixel turn/scale rate mentioned in section \ref{expand} has to be found to use the predictive screen. There is however a way to make the predictive screen work without the need for \textit{any} additional information. By tracking objects in the video a comparison can be made between the predicted movement of objects versus the actual movements. By constantly doing this comparison, the pixel turn/scale rate can be automatically adjusted to minimize the discrepancies. This method can also be used to automatically detect the communication latency. By comparing the time when commands are given and when objects start's to move the delay can be found. Object tracking can be performed using the OpenCV software. This approach would would require a more advanced algorithm and also use more processing power.

